# -*- coding: utf-8 -*-
"""Geo Encoded mean, RF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b8MwhPcnJQYvzdMIMlidVI_mB0JTkpp8
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

train_values = pd.read_csv('train_values.csv', dtype= {'building_id': np.int32,\
                                              'geo_level_1_id': np.int8,\
                                              'geo_level_2_id': np.int16,\
                                              'geo_level_3_id': np.int16,\
                                              'count_floors_pre_eq': np.int16,\
                                              'age': np.int16,\
                                              'area_percentage': np.int8,\
                                              'height_percentage': np.int8,\
                                              'land_surface_condition': 'category',\
                                              'foundation_type': 'category',\
                                              'roof_type': 'category',\
                                              'ground_floor_type':'category',\
                                              'other_floor_type': 'category',\
                                              'position': 'category',\
                                              'plan_configuration':'category',\
                                              'has_superstructure_adobe_mud':'boolean',\
                                              'has_superstructure_mud_mortar_stone':'boolean',\
                                              'has_superstructure_stone_flag':'boolean',\
                                              'has_superstructure_cement_mortar_stone':'boolean',\
                                              'has_superstructure_mud_mortar_brick':'boolean',\
                                              'has_superstructure_cement_mortar_brick':'boolean',\
                                              'has_superstructure_timber':'boolean',\
                                              'has_superstructure_bamboo':'boolean',\
                                              'has_superstructure_rc_non_engineered':'boolean',\
                                              'has_superstructure_rc_engineered':'boolean',\
                                              'has_superstructure_other':'boolean',\
                                              'legal_ownership_status':'category',\
                                              'count_families': np.int16,\
                                              'has_secondary_use':'boolean',\
                                              'has_secondary_use_agriculture':'boolean',\
                                              'has_secondary_use_hotel':'boolean',\
                                              'has_secondary_use_rental':'boolean',\
                                              'has_secondary_use_institution':'boolean',\
                                              'has_secondary_use_school':'boolean',\
                                              'has_secondary_use_industry':'boolean',\
                                              'has_secondary_use_health_post':'boolean',\
                                              'has_secondary_use_gov_office':'boolean',\
                                              'has_secondary_use_use_police':'boolean',\
                                              'has_secondary_use_other':'boolean'
                                              })
train_labels = pd.read_csv("train_labels.csv")
test_values = pd.read_csv("test_values.csv")

columnas_adroppear = ['legal_ownership_status','count_families', 'has_secondary_use_agriculture','has_secondary_use_hotel','has_secondary_use_rental',\
                     'has_secondary_use_institution','has_secondary_use_school','has_secondary_use_industry','has_secondary_use_health_post','has_secondary_use_gov_office',\
                     'has_secondary_use_use_police','has_secondary_use_other']

X = pd.get_dummies(train_values.drop(columns = columnas_adroppear)).drop(columns = 'building_id')
y = train_labels.loc[:,'damage_grade']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

#me quedo con las columnas para los calculos 
encode_geo_train = X_train.merge(y_train.rename("damage_grade"), right_index=True,left_index=True).loc[:,['geo_level_1_id','geo_level_2_id','geo_level_3_id','damage_grade']]
encode_geo_test = X_test.merge(y_test.rename("damage_grade"), right_index=True,left_index=True).loc[:,['geo_level_1_id','geo_level_2_id','geo_level_3_id','damage_grade']]

# cantidad muestras en el training set 
nTR = len(encode_geo_train)
# cantidad muestras en el test set
nTS = len(encode_geo_test)

# weighting function
def peso(n,m):
    return (n)/(m+n)

#cantidad de cada damage_grade

#training_set
nDG1Tr = len(encode_geo_train.loc[encode_geo_train['damage_grade']==1])
nDG2Tr = len(encode_geo_train.loc[encode_geo_train['damage_grade']==2])
nDG3Tr = len(encode_geo_train.loc[encode_geo_train['damage_grade']==3])
#test set
nDG1Ts = len(encode_geo_test.loc[encode_geo_test['damage_grade']==1])
nDG2Ts = len(encode_geo_test.loc[encode_geo_test['damage_grade']==2])
nDG3Ts = len(encode_geo_test.loc[encode_geo_test['damage_grade']==3])

var_total = encode_geo_test['damage_grade'].var()

#geo_level_1
geo_1 = encode_geo_train.loc[:,['geo_level_1_id','damage_grade']]\
        .value_counts().to_frame().reset_index().rename(columns={0:'count'})
#var_total_geo1 = geo_1['damage_grade'].var()
varianzaDG1 = encode_geo_train.loc[:,['geo_level_1_id','damage_grade']].groupby('geo_level_1_id').agg('var').reset_index().rename(columns = {'damage_grade':'var_DG'})
geo1 = geo_1.pivot_table(values=['count'], index=['geo_level_1_id'],columns=['damage_grade'], aggfunc= lambda x: x).fillna(0)
geo1 = geo1.reset_index().droplevel(level=0,axis=1).rename(columns = {1:'count_DG1', 2:'count_DG2',3:'count_DG3'}).drop(columns='')
geo1 = geo1.merge(varianzaDG1, left_index = True, right_on='geo_level_1_id').drop(columns='geo_level_1_id')

#geo_level_2
geo_2 = encode_geo_train.loc[:,['geo_level_2_id','damage_grade']]\
        .value_counts().to_frame().reset_index().rename(columns={0:'count'})
varianzaDG2 = encode_geo_train.loc[:,['geo_level_2_id','damage_grade']].groupby('geo_level_2_id').agg('var').reset_index().rename(columns = {'damage_grade':'var_DG'})
geo2 = geo_2.pivot_table(values=['count'], index=['geo_level_2_id'],columns=['damage_grade'], aggfunc= lambda x: x).fillna(0)
geo2 = geo2.reset_index().droplevel(level=0,axis=1).rename(columns = {1:'count_DG1', 2:'count_DG2',3:'count_DG3'}).drop(columns='')
geo2 = geo2.merge(varianzaDG2, left_index = True, right_on='geo_level_2_id').drop(columns='geo_level_2_id')


#geo_level_3
geo_3 = encode_geo_train.loc[:,['geo_level_3_id','damage_grade']]\
        .value_counts().to_frame().reset_index().rename(columns={0:'count'})
varianzaDG3 = encode_geo_train.loc[:,['geo_level_3_id','damage_grade']].groupby('geo_level_3_id').agg('var').reset_index().rename(columns = {'damage_grade':'var_DG'})
geo3 = geo_3.pivot_table(values=['count'], index=['geo_level_3_id'],columns=['damage_grade'], aggfunc= lambda x: x).fillna(0)
geo3 = geo3.reset_index().droplevel(level=0,axis=1).rename(columns = {1:'count_DG1', 2:'count_DG2',3:'count_DG3'}).drop(columns='')
geo3 = geo3.merge(varianzaDG3, left_index = True, right_on='geo_level_3_id').drop(columns='geo_level_3_id')

#calculos probs psoteriori
#para cada caso puedo calcular solo dos, la tercera puedo calcularla con las dos primeras (1-prob1-prob2)

geo1['prob_post_DG1_geo1'] = geo1['count_DG1']/(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'])
geo1['prob_post_DG2_geo1'] = geo1['count_DG2']/(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'])
geo1['prob_post_DG3_geo1'] = geo1['count_DG3']/(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'])

geo2['prob_post_DG1_geo2'] = geo2['count_DG1']/(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'])
geo2['prob_post_DG2_geo2'] = geo2['count_DG2']/(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'])
geo2['prob_post_DG3_geo2'] = geo2['count_DG3']/(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'])

geo3['prob_post_DG1_geo3'] = geo3['count_DG1']/(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'])
geo3['prob_post_DG2_geo3'] = geo3['count_DG2']/(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'])
geo3['prob_post_DG3_geo3'] = geo3['count_DG3']/(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'])

#geo_1
geo1['est_proba_DG1_geo1'] = geo1['prob_post_DG1_geo1']*peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total) + (1-peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total))*(nDG1Tr/nTR)
geo1['est_proba_DG2_geo1'] = geo1['prob_post_DG2_geo1']*peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total) + (1-peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total))*(nDG2Tr/nTR)
geo1['est_proba_DG3_geo1'] = geo1['prob_post_DG3_geo1']*peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total) + (1-peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total))*(nDG3Tr/nTR)

#geo_2
geo2['est_proba_DG1_geo2'] = geo2['prob_post_DG1_geo2']*peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total) + (1-peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total))*(nDG1Tr/nTR)
geo2['est_proba_DG2_geo2'] = geo2['prob_post_DG2_geo2']*peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total) + (1-peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total))*(nDG2Tr/nTR)
geo2['est_proba_DG3_geo2'] = geo2['prob_post_DG3_geo2']*peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total) + (1-peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total))*(nDG3Tr/nTR)

#geo_3
geo3['est_proba_DG1_geo3'] = geo3['prob_post_DG1_geo3']*peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total) + (1-peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total))*(nDG1Tr/nTR)
geo3['est_proba_DG2_geo3'] = geo3['prob_post_DG2_geo3']*peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total) + (1-peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total))*(nDG2Tr/nTR)
geo3['est_proba_DG3_geo3'] = geo3['prob_post_DG3_geo3']*peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total) + (1-peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total))*(nDG3Tr/nTR)

to_merge_1 = geo1.loc[:,['est_proba_DG1_geo1','est_proba_DG2_geo1','est_proba_DG3_geo1']]
to_merge_2 = geo2.loc[:,['est_proba_DG1_geo2','est_proba_DG2_geo2','est_proba_DG3_geo2']]
to_merge_3 = geo3.loc[:,['est_proba_DG1_geo3','est_proba_DG2_geo3','est_proba_DG3_geo3']]

X_train = X_train.merge(to_merge_1, left_on = 'geo_level_1_id', right_index = True, how = 'left').drop(columns = 'geo_level_1_id')
X_train = X_train.merge(to_merge_2, left_on = 'geo_level_2_id', right_index = True, how = 'left').drop(columns = 'geo_level_2_id')
X_train = X_train.merge(to_merge_3, left_on = 'geo_level_3_id', right_index = True, how = 'left').drop(columns = 'geo_level_3_id')

del geo1
del geo2
del geo3
del geo_1
del geo_2
del geo_3

var_total = encode_geo_test['damage_grade'].var()

#geo_level_1
geo_1 = encode_geo_test.loc[:,['geo_level_1_id','damage_grade']]\
        .value_counts().to_frame().reset_index().rename(columns={0:'count'})
#var_total_geo1 = geo_1['damage_grade'].var()
varianzaDG1 = encode_geo_test.loc[:,['geo_level_1_id','damage_grade']].groupby('geo_level_1_id').agg('var').reset_index().rename(columns = {'damage_grade':'var_DG'})
geo1 = geo_1.pivot_table(values=['count'], index=['geo_level_1_id'],columns=['damage_grade'], aggfunc= lambda x: x).fillna(0)
geo1 = geo1.reset_index().droplevel(level=0,axis=1).rename(columns = {1:'count_DG1', 2:'count_DG2',3:'count_DG3'}).drop(columns='')
geo1 = geo1.merge(varianzaDG1, left_index = True, right_on='geo_level_1_id').drop(columns='geo_level_1_id')

#geo_level_2
geo_2 = encode_geo_test.loc[:,['geo_level_2_id','damage_grade']]\
        .value_counts().to_frame().reset_index().rename(columns={0:'count'})
varianzaDG2 = encode_geo_test.loc[:,['geo_level_2_id','damage_grade']].groupby('geo_level_2_id').agg('var').reset_index().rename(columns = {'damage_grade':'var_DG'})
geo2 = geo_2.pivot_table(values=['count'], index=['geo_level_2_id'],columns=['damage_grade'], aggfunc= lambda x: x).fillna(0)
geo2 = geo2.reset_index().droplevel(level=0,axis=1).rename(columns = {1:'count_DG1', 2:'count_DG2',3:'count_DG3'}).drop(columns='')
geo2 = geo2.merge(varianzaDG2, left_index = True, right_on='geo_level_2_id').drop(columns='geo_level_2_id')


#geo_level_3
geo_3 = encode_geo_test.loc[:,['geo_level_3_id','damage_grade']]\
        .value_counts().to_frame().reset_index().rename(columns={0:'count'})
varianzaDG3 = encode_geo_test.loc[:,['geo_level_3_id','damage_grade']].groupby('geo_level_3_id').agg('var').reset_index().rename(columns = {'damage_grade':'var_DG'})
geo3 = geo_3.pivot_table(values=['count'], index=['geo_level_3_id'],columns=['damage_grade'], aggfunc= lambda x: x).fillna(0)
geo3 = geo3.reset_index().droplevel(level=0,axis=1).rename(columns = {1:'count_DG1', 2:'count_DG2',3:'count_DG3'}).drop(columns='')
geo3 = geo3.merge(varianzaDG3, left_index = True, right_on='geo_level_3_id').drop(columns='geo_level_3_id')

#calculos probs psoteriori
#para cada caso puedo calcular solo dos, la tercera puedo calcularla con las dos primeras (1-prob1-prob2)

geo1['prob_post_DG1_geo1'] = geo1['count_DG1']/(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'])
geo1['prob_post_DG2_geo1'] = geo1['count_DG2']/(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'])
geo1['prob_post_DG3_geo1'] = geo1['count_DG3']/(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'])

geo2['prob_post_DG1_geo2'] = geo2['count_DG1']/(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'])
geo2['prob_post_DG2_geo2'] = geo2['count_DG2']/(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'])
geo2['prob_post_DG3_geo2'] = geo2['count_DG3']/(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'])

geo3['prob_post_DG1_geo3'] = geo3['count_DG1']/(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'])
geo3['prob_post_DG2_geo3'] = geo3['count_DG2']/(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'])
geo3['prob_post_DG3_geo3'] = geo3['count_DG3']/(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'])

#geo_1
geo1['est_proba_DG1_geo1'] = geo1['prob_post_DG1_geo1']*peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total) + (1-peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total))*(nDG1Ts/nTS)
geo1['est_proba_DG2_geo1'] = geo1['prob_post_DG2_geo1']*peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total) + (1-peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total))*(nDG2Ts/nTS)
geo1['est_proba_DG3_geo1'] = geo1['prob_post_DG3_geo1']*peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total) + (1-peso(geo1['count_DG1']+geo1['count_DG2']+geo1['count_DG3'],geo1['var_DG']/var_total))*(nDG3Ts/nTS)

#geo_2
geo2['est_proba_DG1_geo2'] = geo2['prob_post_DG1_geo2']*peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total) + (1-peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total))*(nDG1Ts/nTS)
geo2['est_proba_DG2_geo2'] = geo2['prob_post_DG2_geo2']*peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total) + (1-peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total))*(nDG2Ts/nTS)
geo2['est_proba_DG3_geo2'] = geo2['prob_post_DG3_geo2']*peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total) + (1-peso(geo2['count_DG1']+geo2['count_DG2']+geo2['count_DG3'],geo2['var_DG']/var_total))*(nDG3Ts/nTS)

#geo_3
geo3['est_proba_DG1_geo3'] = geo3['prob_post_DG1_geo3']*peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total) + (1-peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total))*(nDG1Ts/nTS)
geo3['est_proba_DG2_geo3'] = geo3['prob_post_DG2_geo3']*peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total) + (1-peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total))*(nDG2Ts/nTS)
geo3['est_proba_DG3_geo3'] = geo3['prob_post_DG3_geo3']*peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total) + (1-peso(geo3['count_DG1']+geo3['count_DG2']+geo3['count_DG3'],geo3['var_DG']/var_total))*(nDG3Ts/nTS)

to_merge_1 = geo1.loc[:,['est_proba_DG1_geo1','est_proba_DG2_geo1','est_proba_DG3_geo1']]
to_merge_2 = geo2.loc[:,['est_proba_DG1_geo2','est_proba_DG2_geo2','est_proba_DG3_geo2']]
to_merge_3 = geo3.loc[:,['est_proba_DG1_geo3','est_proba_DG2_geo3','est_proba_DG3_geo3']]

X_test = X_test.merge(to_merge_1, left_on = 'geo_level_1_id', right_index = True, how = 'left').drop(columns = 'geo_level_1_id')
X_test = X_test.merge(to_merge_2, left_on = 'geo_level_2_id', right_index = True, how = 'left').drop(columns = 'geo_level_2_id')
X_test = X_test.merge(to_merge_3, left_on = 'geo_level_3_id', right_index = True, how = 'left').drop(columns = 'geo_level_3_id')

X_test.fillna(0, inplace = True)
X_train.fillna(0, inplace = True)

#hiper para rf

hiper = {"class_weight":['balanced'], "n_estimators":np.arange(100,1000,100),
         "max_features" :['auto', 'sqrt','log2'], "max_depth":np.arange(10,200,10), 
         "bootstrap": [True, False]}
rf_clf = RandomForestClassifier()
gd_sr = GridSearchCV(rf_clf,param_grid=hiper,scoring='f1_micro',cv=5)

gd_sr.fit(X_train, y_train)

print(gd_sr.best_score_)
print(gd_sr.best_params_)

